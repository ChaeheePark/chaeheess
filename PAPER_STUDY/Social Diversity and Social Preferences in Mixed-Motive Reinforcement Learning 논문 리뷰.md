# Social Diversity and Social Preferences in Mixed-Motive Reinforcement Learning 논문 리뷰

## ABSTRACT

pure-conflict와 pure-common interest game의 강화 학습에 대한 최근 연구는 인구 이질성의 중요성을 강조 <-> **반대로, mixed-motive game의 강화 학습에 대한 연구는 동종 접근법을 활용**

mixed-motive game의 정의 특성(그룹 구성원 간의 인센티브의 불완전한 상관 관계)을 고려할 때, **mixed-motive game의 강화 학습에 대한 모집단 이질성의 영향을 연구해야함**

사회 심리학에서 상호의존 이론을 도출하고 강화 학습 에이전트를 그룹 결과 분포에 대한 선호도의 공식인 사회적 가치 지향(SVO)으로 대입하고, 이어서 2 mixed-motive Markov game에서 강화 학습 에이전트 모집단에 대한 SVO의 다양성의 영향을 탐구

SVO의 이질성이 상호의존 이론에 의해 제안된 것과 유사한 에이전트 간에 의미 있고 복잡한 행동 변화를 발생-> 이러한 mixed-motive 딜레마에 대한 경험적 결과는 **이종 모집단에서 훈련된 agent가 동종 모집단에서 훈련된 agent비교하여 특히 일반화된 고성능 정책을 개발함**



## 1 INTRODUCTION

다중 에이전트 강화 학습에서, 한 에이전트의 작업은 다른 에이전트의 경험과 결과에 영향을 미칠 수 있음->에이전트는 상호 의존적

상호의존적 상호작용은 관련된 에이전트에 대한 인센티브의 정렬에 따라 두 가지 범주로 분류될 수 있음

(1) Pure-motive interaction: 그룹의 인센티브가 완전히 일치 (순수한 공통 관심사) / 완전히 반대 (순수한 갈등) 
(2) mixed-motive interaction: 그룹의 인센티브가 때로는 일치하고 때로는 반대되는 혼합된 상호 작용

이러한 범주적 구분은 특히 공간적 및 시간적으로 확장된 Markov game과 관련됨

<img width="242" alt="image" src="https://user-images.githubusercontent.com/60170358/169747686-0b1155da-31f5-4917-b36e-b8fcc17025a2.png">

모집단 기반 다중 에이전트 강화 학습의 동질성과 이질성

(a) 모집단의 동질성과 이질성은 주어진 에이전트 i에 대해 다른 훈련 경험을 초래->동종의 경우 에이전트 정책이 동일하거나 매우 유사 / 이종 설정에서 주어진 에이전트 i는 시간이 지남에 따라 다양한 그룹 

(b) 이전 다중 에이전트 강화 학습 연구의 대표적인 예



pure-conflict 강화학습에서, Markov game의 self-play solution은 점차 모집단 기반 접근에 집중(상대 이질성에 대해 에이전트 성능을 견고하게 보장하는 데 관심) 

Markov game의 pure-common interest 강화 학습에 대한 최근 연구는 다양한 파트너 정책에 대한 견고성의 중요성을 강조 

-> 이러한 두 가지 상황에서, 광범위한 잠재적 정책 세트에 적응하고 가장 잘 대응할 수 있는 에이전트를 교육하는 것이 나음

**mixed-motive** Markov game에서, 파트너 이질성의 효과가 큰 주목을 받지 못하고 대부분의 mixed-motive 강화 학습 연구는 고정된 그룹의 self-play 또는 co-training 정책을 통해 만듬-> 이러한 방법은 각 에이전트가 접하는 정책 집합의 동질성을 촉진

**->mixed-motive 강화 학습에 정책 이질성을 도입하는 것을 목표**

이질성 연구의 자연스러운 출발점은 내재적 동기 부여에서 다양성의 영향을 탐구하는 것-> 여기서 우리는 에이전트에게 자신과 다른 사람 사이의 특정 그룹 보상 분포를 선호하는 본질적인 동기인 사회적 가치 지향(SVO)을 부여

심리학 및 경제학 연구는 인간 그룹이 이질성 분포 선호도를 통해 서로 다른 게임에 걸쳐 높은 수준의 협력을 지속한다는 것을 반복적으로 입증 

-> 상호의존성 이론에서 각 플레이어가 게임의 "주어진 매트릭스"에 따라 행동하는 것이 아니라 결과 집합을 주관적으로 평가되어 나타내는 "효과 매트릭스"에 따라 행동하기 때문에 경제 게임에서 인간이 게임 이론적 예측에서 벗어남, 플레이어는 자신의 환경에서 "주어진 매트릭스"를 받은 후 다양한 결과 분포에 대한 개인의 선호도를 반영하는 "결과 변환"을 적용

<img width="299" alt="image" src="https://user-images.githubusercontent.com/60170358/169757666-f6739eda-5e0b-4e77-b328-563994b67567.png">

주어진 매트릭스와 결과 변환의 조합이 효과 매트릭스를 형성 : 상호의존 이론- 위 경로는 **이타적인 선호**를 가진 행 플레이어의 변환 과정을 묘사, 이 경우 결과 변환은 열 플레이어의 보상을 효과 매트릭스로 직접 전달, 아래쪽 경로는 **경쟁적 선호도**를 가진 행 플레이어의 변환 과정을 보여주며, 행 플레이어는 자신의 payoff와 열 플레이어의 payoff 거리를 최대화하는 것-> 이 두 가지 결과 변환은 서로 다른 지배적 전략을 제안

개인의 사회적 선호도가 한 게임에서 주어진 보상을 감소시킬 수 있지만, 다양한 선호도를 가진 집단은 차선의 Nash equilibria에 저항할 수 있음

다중 에이전트 강화 학습에서 보상 공유는 일반적으로 mixed-motive 딜레마를 해결하는 데 사용 (지금까지 보상 혼합물을 제어하는 에이전트 하이퍼 파라미터는 일반적으로 공유됨, 이 접근법은 경제학의 대표 에이전트 가정을 반영하여 균일한 정책 집단을 의미)

이질성을 포착하는 능력이 다른 모델링 접근 방식에 비해 에이전트 기반 모델의 주요 강점임을 고려할 때, **공유 보상 혼합물에 대한 지속적인 의존도**는 특히 두드러짐

동종 집단은 종종 게으른 에이전트 문제의 특수한 변형에 희생양이 되는데, 여기서 하나 이상의 에이전트는 당면한 개별 학습 과제를 무시하고 대신 공유 보상을 위해 최적화, 이러한 공유 보상 담당자들은 친사회적 작업의 부담을 짊어지며, 에피소드 전반에 걸친 집단 구성의 급격한 변화에 불변-> 이러한 "전문화"는 일반화된 정책을 생성하기 위한 교육의 실패를 의미

혼합 동기 강화 학습에서 이질성의 영향을 조사하기 위해, 우리는 보상 공유를 위한 새롭고 일반화된 메커니즘을 도입->보상 공유 메커니즘인 사회적 가치 지향(SVO)을 사회 심리학에서 인간의 협력 행동에 대한 연구에서 도출

->여러 게임에 걸쳐 그룹 내에서 이러한 사회적 선호도의 이기종 분포가 동종 분포보다 더 일반화된 개별 정책을 생성한다는 것을 보여주고 SVO의 이질성이 어떻게 긍정적인 그룹 결과를 유지하는지 탐구-> 우리는 사회적 선호의 이러한 공식화가 에이전트가 각 환경과 관련된 특정 친사회적 행동을 발견하도록 이끈다는 것을 보여줌



## 2 AGENTS

### 2.1  Multi-agent reinforcement learning and Markov games

n-player observable Markov game을 고려

유한집합 S로 정의

관찰 함수 O: S × {1, ...,n} → R^d

각 플레이어에 대해 사용 가능한 동작 집합 A1, ., An

N 플레이어에 의해 취해진 공동 동작으로부터 이산 분포의 집합으로 매핑되는 확률적 전이 함수 T: S × A1 × · · · · An → ( (S)가 부여

각 주에서 플레이어는 a= (a1, ..., an) → A1, ..., ..., An을 공동 액션을

각 에이전트 i는 독립적으로 환경을 경험하고 행동 정책 α(ai |oi)를 학습(자체 관찰 oi = O(s,i) 및  외적 보상 ri(s,a®)를 기반)

장기 y-discounted payoff(식 1에 정의된)를 최대화 하기 위해 agent는 학습됨



<img width="228" alt="image" src="https://user-images.githubusercontent.com/60170358/170282627-f9b91890-e1c5-4575-8a1c-0ecd4d2f2470.png"> (식1)

여기서 Ui(st, ot, at)는 utility function이며, 단순성을 위해 o=(o1,..,on)

표준 강화 학습에서 utility function은 환경에 의해 제공되는 외적 보상에 직접 매핑됨