# Social Diversity and Social Preferences in Mixed-Motive Reinforcement Learning 논문 리뷰

## ABSTRACT

pure-conflict와 pure-common interest game의 강화 학습에 대한 최근 연구는 인구 이질성의 중요성을 강조 <-> **반대로, mixed-motive game의 강화 학습에 대한 연구는 동종 접근법을 활용**

mixed-motive game의 정의 특성(그룹 구성원 간의 인센티브의 불완전한 상관 관계)을 고려할 때, **mixed-motive game의 강화 학습에 대한 모집단 이질성의 영향을 연구해야함**

사회 심리학에서 상호의존 이론을 도출하고 강화 학습 에이전트를 그룹 결과 분포에 대한 선호도의 공식인 사회적 가치 지향(SVO)으로 대입하고, 이어서 2 mixed-motive Markov game에서 강화 학습 에이전트 모집단에 대한 SVO의 다양성의 영향을 탐구

SVO의 이질성이 상호의존 이론에 의해 제안된 것과 유사한 에이전트 간에 의미 있고 복잡한 행동 변화를 발생-> 이러한 mixed-motive 딜레마에 대한 경험적 결과는 **이종 모집단에서 훈련된 agent가 동종 모집단에서 훈련된 agent비교하여 특히 일반화된 고성능 정책을 개발함**



## 1 INTRODUCTION

다중 에이전트 강화 학습에서, 한 에이전트의 작업은 다른 에이전트의 경험과 결과에 영향을 미칠 수 있음->에이전트는 상호 의존적

상호의존적 상호작용은 관련된 에이전트에 대한 인센티브의 정렬에 따라 두 가지 범주로 분류될 수 있음

(1) Pure-motive interaction: 그룹의 인센티브가 완전히 일치 (순수한 공통 관심사) / 완전히 반대 (순수한 갈등) 
(2) mixed-motive interaction: 그룹의 인센티브가 때로는 일치하고 때로는 반대되는 혼합된 상호 작용

이러한 범주적 구분은 특히 공간적 및 시간적으로 확장된 Markov game과 관련됨

<img width="242" alt="image" src="https://user-images.githubusercontent.com/60170358/169747686-0b1155da-31f5-4917-b36e-b8fcc17025a2.png">

모집단 기반 다중 에이전트 강화 학습의 동질성과 이질성

(a) 모집단의 동질성과 이질성은 주어진 에이전트 i에 대해 다른 훈련 경험을 초래->동종의 경우 에이전트 정책이 동일하거나 매우 유사 / 이종 설정에서 주어진 에이전트 i는 시간이 지남에 따라 다양한 그룹 

(b) 이전 다중 에이전트 강화 학습 연구의 대표적인 예



pure-conflict 강화학습에서, Markov game의 self-play solution은 점차 모집단 기반 접근에 집중(상대 이질성에 대해 에이전트 성능을 견고하게 보장하는 데 관심) 

Markov game의 pure-common interest 강화 학습에 대한 최근 연구는 다양한 파트너 정책에 대한 견고성의 중요성을 강조 

-> 이러한 두 가지 상황에서, 광범위한 잠재적 정책 세트에 적응하고 가장 잘 대응할 수 있는 에이전트를 교육하는 것이 나음

**mixed-motive** Markov game에서, 파트너 이질성의 효과가 큰 주목을 받지 못하고 대부분의 mixed-motive 강화 학습 연구는 고정된 그룹의 self-play 또는 co-training 정책을 통해 만듬-> 이러한 방법은 각 에이전트가 접하는 정책 집합의 동질성을 촉진

**->mixed-motive 강화 학습에 정책 이질성을 도입하는 것을 목표**

이질성 연구의 자연스러운 출발점은 내재적 동기 부여에서 다양성의 영향을 탐구하는 것-> 여기서 우리는 에이전트에게 자신과 다른 사람 사이의 특정 그룹 보상 분포를 선호하는 본질적인 동기인 사회적 가치 지향(SVO)을 부여

심리학 및 경제학 연구는 인간 그룹이 이질성 분포 선호도를 통해 서로 다른 게임에 걸쳐 높은 수준의 협력을 지속한다는 것을 반복적으로 입증 

-> 상호의존성 이론에서 각 플레이어가 게임의 "주어진 매트릭스"에 따라 행동하는 것이 아니라 결과 집합을 주관적으로 평가되어 나타내는 "효과 매트릭스"에 따라 행동하기 때문에 경제 게임에서 인간이 게임 이론적 예측에서 벗어남, 플레이어는 자신의 환경에서 "주어진 매트릭스"를 받은 후 다양한 결과 분포에 대한 개인의 선호도를 반영하는 "결과 변환"을 적용

<img width="299" alt="image" src="https://user-images.githubusercontent.com/60170358/169757666-f6739eda-5e0b-4e77-b328-563994b67567.png">

주어진 매트릭스와 결과 변환의 조합이 효과 매트릭스를 형성 : 상호의존 이론- 위 경로는 **이타적인 선호**를 가진 행 플레이어의 변환 과정을 묘사, 이 경우 결과 변환은 열 플레이어의 보상을 효과 매트릭스로 직접 전달, 아래쪽 경로는 **경쟁적 선호도**를 가진 행 플레이어의 변환 과정을 보여주며, 행 플레이어는 자신의 payoff와 열 플레이어의 payoff 거리를 최대화하는 것-> 이 두 가지 결과 변환은 서로 다른 지배적 전략을 제안

개인의 사회적 선호도가 한 게임에서 주어진 보상을 감소시킬 수 있지만, 다양한 선호도를 가진 집단은 차선의 Nash equilibria에 저항할 수 있음

다중 에이전트 강화 학습에서 보상 공유는 일반적으로 mixed-motive 딜레마를 해결하는 데 사용 (지금까지 보상 혼합물을 제어하는 에이전트 하이퍼 파라미터는 일반적으로 공유됨, 이 접근법은 경제학의 대표 에이전트 가정을 반영하여 균일한 정책 집단을 의미)

이질성을 포착하는 능력이 다른 모델링 접근 방식에 비해 에이전트 기반 모델의 주요 강점임을 고려할 때, **공유 보상 혼합물에 대한 지속적인 의존도**는 특히 두드러짐

동종 집단은 종종 게으른 에이전트 문제의 특수한 변형에 희생양이 되는데, 여기서 하나 이상의 에이전트는 당면한 개별 학습 과제를 무시하고 대신 공유 보상을 위해 최적화, 이러한 공유 보상 담당자들은 친사회적 작업의 부담을 짊어지며, 에피소드 전반에 걸친 집단 구성의 급격한 변화에 불변-> 이러한 "전문화"는 일반화된 정책을 생성하기 위한 교육의 실패를 의미

혼합 동기 강화 학습에서 이질성의 영향을 조사하기 위해, 우리는 보상 공유를 위한 새롭고 일반화된 메커니즘을 도입->보상 공유 메커니즘인 사회적 가치 지향(SVO)을 사회 심리학에서 인간의 협력 행동에 대한 연구에서 도출

->여러 게임에 걸쳐 그룹 내에서 이러한 사회적 선호도의 이기종 분포가 동종 분포보다 더 일반화된 개별 정책을 생성한다는 것을 보여주고 SVO의 이질성이 어떻게 긍정적인 그룹 결과를 유지하는지 탐구-> 우리는 사회적 선호의 이러한 공식화가 에이전트가 각 환경과 관련된 특정 친사회적 행동을 발견하도록 이끈다는 것을 보여줌



## 2 AGENTS

### 2.1  Multi-agent reinforcement learning and Markov games

n-player observable Markov game을 고려

유한집합 S로 정의

관찰 함수 O: S × {1, ...,n} → R^d

각 플레이어에 대해 사용 가능한 동작 집합 A1, ., An

N 플레이어에 의해 취해진 공동 동작으로부터 이산 분포의 집합으로 매핑되는 확률적 전이 함수 T: S × A1 × · · · · An → ( (S)가 부여

각 주에서 플레이어는 a= (a1, ..., an) → A1, ..., ..., An을 공동 액션을

각 에이전트 i는 독립적으로 환경을 경험하고 행동 정책 α(ai |oi)를 학습(자체 관찰 oi = O(s,i) 및  외적 보상 ri(s,a®)를 기반)

장기 y-discounted payoff(식 1에 정의된)를 최대화 하기 위해 agent는 학습됨



<img width="228" alt="image" src="https://user-images.githubusercontent.com/60170358/170282627-f9b91890-e1c5-4575-8a1c-0ecd4d2f2470.png"> (식1)

여기서 Ui(st, ot, at)는 utility function이며, 단순성을 위해 o=(o1,..,on)

표준 강화 학습에서 utility function은 환경에 의해 제공되는 외적 보상에 직접 매핑됨



### 2.2 Social Value Orientation

여기서는 자신과 다른 사람 사이의 특정 그룹 보상 분포를 선호하는 본질적인 동기인 사회적 가치 지향(SVO)을 소개합니다.
우리는 플레이어 i와 그룹의 모든 다른 플레이어 간에 관찰된 보상 분포의 스칼라 표현으로 보상 각도의 개념을 도입합니다. 이 두 스칼라에 의해 형성된 각도의 크기는 자신과 타인 사이의 상대적 보상 분포를 나타냅니다(그림 3).

<img width="253" alt="image" src="https://user-images.githubusercontent.com/60170358/170424532-16717e3e-101f-4798-95dd-0f7b4c8cbf93.png">

보상 각도와 사회적 가치 지향(SVO)의 ring formulation

보상 각도: 에이전트 자신의 보상과 환경에 있는 다른 에이전트의 보상 사이의 균형을 나타내는 스칼라 표현  에이전트가 선호하는 보상 각도는 SVO입니다.

크기가 n인 그룹이 주어지면, 그에 상응하는 보상 벡터는 R®= (r1, . . . ,rn)입니다. 플레이어 i의 보상 각도는 다음과 같습니다.

### 2.3 Algorithm





## 3 MIXED-MOTIVE GAMES

### 3.1 Intertemporal social dilemmas

그룹 크기 n = 5로 플레이되는 두 개의 시간적 및 공간적으로 확장된 혼합된 게임인 HavestPatch와 Cleanup을 고려-> mixed-motive Markov game의 특정 부류인 시간적 사회적 딜레마

시간적 사회적 딜레마는 단기적 개인 인센티브와 장기적 집단 이익 사이의 긴장을 제시하는 집단적 상황

각 개인은 친사회적(협력적) 또는 이기적(탈주적)으로 행동할 수 있는 선택권을 가짐

만장일치의 협력이 장기적으로 복지를 극대화하는 결과를 낳음에도 불구하고, 짧은 시간 내에 이기적으로 행동하는 것의 개인적 이익이 친사회적 행동의 이익을 엄격하게 지배-> 비록 그룹의 모든 구성원들이 상호 협력의 보상을 선호하지만, 시간 간 인센티브 구조는 단체들을 복지를 억제하는 균형으로 압박



### 3.2 HarvestPatch

HarvestPatch는 common-pool 리소스 전용 게임 Harvest의 변형(그림 a)

<img width="551" alt="image" src="https://user-images.githubusercontent.com/60170358/170504099-1b1abdbd-9055-4140-988c-4a52cc1e2512.png">

플레이어는 24×26 그리드 월드 환경에서 사과(리워드 +1)를 수집하면 보상을 받음

사과는 재배 반경이 3인 범위 내에서 수확되지 않은 사과의 수에 따라 수확된 후 재생됩니다. 만약 사과의 반지름 안에 사과가 없다면, 사과는 다시 자랄 수 없습니다. 각 에피소드가 시작될 때, 사과는 아마도 6각형의 패치 패턴으로 생성되는데, 따라서 각 사과는 패치의 다른 모든 사과의 재생 반경 내에 있고 다른 모든 패치의 재생 반경 밖에 있습니다. 이렇게 하면 각 애플 패치에 대해 현지화된 재고 및 흐름 속성 [18]이 생성됩니다. 각 패치는 다른 패치에 남아 있는 사과 수에 관계없이 모든 사과가 수확되면 되돌릴 수 없이 고갈됩니다. 플레이어는 또한 빔을 사용하여 적은 비용으로 다른 플레이어(리워드 - 50)를 벌할 수 있습니다(리워드 -1). 이를 통해 처벌을 사용하여 무임승차를 억제
그룹은 "멸종 위기에 처한 사과"를 먹지 않음으로써 무기한 지속 가능한 수확을 달성할 수 있습니다. 하지만, 지속가능한 수확에 대한 보상은 모든 선수들이 기권하는 경우에만 재성장 기간이 지난 후에 나타납니다. 이와는 대조적으로, 개인이 탐욕스럽게 행동하면 위험에 처한 사과를 먹은 것에 대한 보상을 즉시 그리고 일방적으로 보장받습니다. 이것은 지속 불가능한 행동을 통해 보상을 최대화하려는 단기적인 개인의 유혹과 지속 가능한 행동을 통해 더 높은 보상을 창출하는 장기적 집단의 이익과 병행하여 딜레마를 만듭니다.
하베스트 패치에서는 에피소드가 1000단계까지 진행, 각 에이전트의 관찰 가능 여부는 현재 위치를 중심으로 15 × 15 RGB 창으로 제한됩니다. 동작 공간은 벌 빔의 이동, 회전 및 사용으로 구성됩니다(총 8 동작).



### 3.3 Cleanup

Cleanup은 공공 게임(그림 b)

플레이어는 25 × 18 그리드 세계 환경에서 사과(리워드 +1)를 수집하면 다시 보상을 받음

Cleanup에서 사과는 근처 강의 청결도와 반비례하는 비율로 과수원에서 자랍니다. 그 강은 시간이 지남에 따라 일정한 확률로 오염을 축적합니다. 특정 오염의 문턱을 넘어서면, 과수원의 사과 성장률은 0으로 떨어집니다. 플레이어들은 강에서 소량의 오염을 제거할 수 있는 추가적인 행동을 합니다. 하지만, Cleanup 작업은 에이전트들 앞에 있는 작은 거리 내의 오염에만 작용하여, 그들이 강을 청소하기 위해 물리적으로 사과 과수원을 떠날 것을 요구합니다. 그러므로, 선수들은 노력적인 기부를 통해 과수원 재생의 공익성을 유지합니다.
하베스트 패치와 마찬가지로 플레이어는 빔을 사용하여 적은 비용으로 다른 플레이어(리워드 - 50)를 벌할 수 있습니다(리워드 - 1).
한 집단이 시간이 지남에 따라 강의 오염도를 지속적으로 낮게 유지함으로써 과수원에서 지속적인 사과 성장을 이룰 수 있습니다. 하지만, 짧은 시간 동안, 다른 선수들이 강에서 공공의 이익을 제공하는 동안, 각 선수들은 과수원에서 사과를 모으는 것을 선호합니다. 이는 과수원에 머물면서 보상을 극대화하기 위한 단기적 개인 인센티브와 시간이 지남에 따라 지속적인 기여를 통해 공공의 이익을 유지하려는 장기적 집단 이익 사이에 긴장을 조성합니다.
에피소드는 1000단계까지 진행,  에이전트 관찰 가능 여부는 다시 에이전트의 현재 위치를 중심으로 15 × 15 RGB 창으로 제한됩니다. Cleanup의 경우 에이전트는 정리에 대한 추가 수행(9가지 수행)이 있습니다.



## 4 RESULTS

### 4.1 Social diversity and agent generality

