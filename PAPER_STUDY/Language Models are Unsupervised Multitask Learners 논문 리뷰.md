# Language Models are Unsupervised Multitask Learners 논문 리뷰

## Abstract

NLP task는 전형적으로 task에 맞는 dataset으로 학습함-> 본 논문에서는  WebText라는 수백만 웹 페이지의 데이터로 explicit supervision 없이 학습함

language model의 용량은 zero-shot task가 필수적이고 log-linear performance로 성능이 향상

GPT-2는 1.5B의 parameter의 Transformer이고 그것은 8개중 7개의 state를 zero-shot setting을 하여 state of art를 달성함

유망한 자연어 처리 시스템을 구축



## Introduction

