# Autoencoders(Dor Bank, Noam Koenigstein, Raja Giryes) ë…¼ë¬¸ ë¦¬ë·°

## Abstract

**autoencoderëŠ” ì…ë ¥ì„ ì••ì¶•ë˜ê³  ì˜ë¯¸ ìˆëŠ” í‘œí˜„ìœ¼ë¡œ encoding í•œ ë‹¤ìŒ, ì¬êµ¬ì„±ëœ ì…ë ¥ì´ ê°€ëŠ¥í•œ ì›ë˜ ì…ë ¥ê³¼ ë¹„ìŠ·í•˜ë„ë¡ ë‹¤ì‹œ decoding í•˜ë„ë¡ ì„¤ê³„ëœ ì‹ ê²½ë§**



## 1 Autoencoders	

inputì„ reconstructì„ í•˜ë„ë¡ í•™ìŠµì‹œí‚¤ëŠ” neural network

-> clusteringê°™ì€ ë‹¤ì–‘í•œ ì˜ë¯¸ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì˜ "informative" í‘œí˜„ì„ unsupervised ë°©ì‹ìœ¼ë¡œ í•™ìŠµ

<img width="262" alt="image" src="https://user-images.githubusercontent.com/60170358/172117756-63ba7902-ece3-4cee-af62-709cf11db790.png">A-> 

A: input xë¥¼ encoderì— í†µê³¼ì‹œí‚¨ ê°’ <img width="85" alt="image" src="https://user-images.githubusercontent.com/60170358/172118129-545f1bcb-36ba-411a-9e32-11c958f7d9ce.png">

B: encoderì— í†µê³¼ì‹œí‚¨ ê°’ì„ decoderì— í†µê³¼ì‹œí‚¨ ê°’ <img width="83" alt="image" src="https://user-images.githubusercontent.com/60170358/172118170-65aacefc-c4d0-44d7-bc07-dc15eaad35b8.png">

output of the decoder ê³¼ input x ì‚¬ì´ì˜ lossë¥¼ ìµœì†Œí™” í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµë¨


<img width="355" alt="image" src="https://user-images.githubusercontent.com/60170358/172118313-4bbf90ab-24b0-46c4-b403-40efb8535f98.png">

autoencoderëŠ” end-to-end ë˜ëŠ” layer-by-layerë¡œ í•™ìŠµë¨



## 2 Regularized autoencoders

training ë™ì•ˆ, Aì™€ Bì— ëŒ€í•œ identityì—°ì‚°ìë¥¼ ì–»ì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, achieved representationì„ ì…ë ¥ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€í•˜ë ¤ë©´ ì¶”ê°€ì ì¸ ì •ê·œí™”ê°€ í•„ìš”

ê°€ì¥ í”í•œ ë°©ë²•ì€ representationì˜ ì°¨ì›ì„ inputë³´ë‹¤ ì‘ê²Œ ë§Œë“œëŠ” ë°©ë²• (ë³‘ëª©í˜„ìƒì´ ìƒê¹€)

hidden layerì˜ í¬ê¸°ê°€ inputì˜ í¬ê¸°ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ì€ ê²½ìš° encoderê°€ ë‹¨ìˆœíˆ identity í•¨ìˆ˜ë¥¼ í•™ìŠµí•  ìœ„í—˜ì´ ìˆìŒ-> ë³‘ëª© í˜„ìƒì„ ë§Œë“¤ì§€ ì•ŠëŠ” ì •ê·œí™”ë¥¼ ìœ„í•œ ëª‡ ê°€ì§€ ì˜µì…˜ì´ ì¡´ì¬

autoencoderì—ì„œ bias-variance tradeoffê°€ ì¤‘ìš”-> autoencoderì˜ ì•„í‚¤í…ì²˜ê°€ inputì„ ì˜ reconstructí•  ìˆ˜ ìˆê²Œ ì›í•˜ì§€ë§Œ, low representationì´ ì˜ë¯¸ìˆëŠ” ê²ƒìœ¼ë¡œ ì¼ë°˜í™”ë˜ê¸°ë¥¼ ì›í•¨



### 2.1 Sparse Autoencoders

tradeoffë¥¼ ì²˜ë¦¬í•˜ëŠ” í•œê°€ì§€ ë°©ë²•ì€ hidden activationì— í¬ì†Œì„±ì„ ì ìš©í•˜ëŠ” ê²ƒ-> ì´ ê¸°ëŠ¥ì€ ë³‘ëª© í˜„ìƒ ì ìš© ìœ„ì— ì¶”ê°€í•˜ê±°ë‚˜ ë³‘ëª© í˜„ìƒ ëŒ€ì‹  ì¶”ê°€

1) **L1 regularization ì ìš©**

<img width="373" alt="image" src="https://user-images.githubusercontent.com/60170358/172127096-b9e12875-ec34-4f63-806e-b3d21c9b661c.png">

â€‹	autoencoderì˜ objectiveì€ ì‹(2)ì™€ ê°™ê²Œ ë¨

â€‹	<img width="27" alt="image" src="https://user-images.githubusercontent.com/60170358/172127303-09e1d247-0e83-4beb-a37c-e4458284203e.png">ëŠ” hidden layerì˜ ië²ˆì§¸ activation

2. **KL-divergence**

   ë‘ í™•ë¥  ë¶„í¬ ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•œ ê²ƒ-> L1 ì •ê·œí™”ì—ì„œ lambdaë¥¼ ì¡°ì •í•˜ëŠ” ëŒ€ì‹ , ê° neuronì˜ activationì´ í™•ë¥  Î±ë¥¼ ê°–ëŠ” ë² ë¥´ëˆ„ì´ ë³€ìˆ˜ë¡œ ì‘ìš©í•œë‹¤ê³  ê°€ì •í•˜ê³  í•´ë‹¹ í™•ë¥ ì„ ì¡°ì •í•˜ì—¬ ê° batchì—ì„œ ì‹¤ì œ í™•ë¥ ì´ ì¸¡ì •ë˜ê³  ì°¨ì´ê°€ ê³„ì‚°ë˜ì–´ regularization factorë¡œ ì ìš©

   <img width="311" alt="image" src="https://user-images.githubusercontent.com/60170358/172128354-f729f39e-1c0d-4c25-8ae6-10c9d07749fc.png">

   ê° neuron jì— ê³„ì‚°ëœ ê²½í—˜ì  í™•ë¥ ì€ <img width="118" alt="image" src="https://user-images.githubusercontent.com/60170358/172128520-81685338-ff8b-4482-a23e-057e55705fdc.png">

   pì™€ <img width="22" alt="image" src="https://user-images.githubusercontent.com/60170358/172129371-47b2be7d-22f2-4eeb-956d-1e441f37da70.png">ì„ ê°™ê²Œí•˜ëŠ”ê²ƒì´ regularization termì˜ ëª©ì 



### 2.2 Denoising Autoencoders

<img width="346" alt="image" src="https://user-images.githubusercontent.com/60170358/172129494-36eed890-33aa-4fd5-83d1-de3204a49bdd.png">

Denoising Autoencoderì€ regularization option ë˜ëŠ” error correctionìœ¼ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ autoencoder-> inputì€ ì¼ë¶€ ë…¸ì´ì¦ˆ(white Gaussian noise ë“±)ê°€ ì¶”ê°€ë¨

<img width="31" alt="image" src="https://user-images.githubusercontent.com/60170358/172153770-bb2d073a-f768-4880-87a1-2285ce0bfcb0.png"> ëŠ” random ë³€ìˆ˜ì´ê³ , <img width="36" alt="image" src="https://user-images.githubusercontent.com/60170358/172153829-2b453438-a07b-416a-8f51-ba14343a338c.png">ì˜ ë¶„í¬ë¥¼ ë”°ë¦„



<img width="353" alt="image" src="https://user-images.githubusercontent.com/60170358/172153167-8f73f263-709e-497f-b424-05ba4eca9225.png">

ì‹ (4)ëŠ” <img width="25" alt="image" src="https://user-images.githubusercontent.com/60170358/172153281-47c64966-6891-4ef4-a534-caa0e216d110.png">ê°€ noiseì˜ ì •ë„ë¥¼ ê²°ì •

ì‹ (5)ëŠ” pê°€ xì˜ ê°’ì´ ë¬´íš¨í™”ë˜ì§€ ì•Šì„ í™•ë¥ ì„ ì„¤ì •



### 2.3 Contractive Autoencoders

Contractive autoencodersì—ì„œ, decoderì— ì˜í•œ reconstructionì— ì¤‘ìš”í•˜ì§€ ì•Šì€ inputì˜ changesë¥¼ ë¬´ì‹œí•˜ë„ë¡ í•´ì„œ feature extractionì´ ì‘ì€ ë³€í™”ì— ëœ ë¯¼ê°í•˜ê²Œ ë§Œë“œëŠ” ë° ì¤‘ì 

ë”°ë¼ì„œ ë„¤íŠ¸ì›Œí¬ì˜ Jacobianì— ëŒ€í•´ penaltyê°€ ë¶€ê³¼

Jacobian matrixì˜ hidden layer hëŠ” ì…ë ¥ xì˜ ê° ê°’ xiì— ëŒ€í•œ ê° ë…¸ë“œ hjì˜ ë¯¸ë¶„ë“¤ë¡œ êµ¬ì„±ë¨-><img width="107" alt="image" src="https://user-images.githubusercontent.com/60170358/172155386-108c84c8-94ab-4362-8652-cc2e9b334dbb.png">

<img width="337" alt="image" src="https://user-images.githubusercontent.com/60170358/172155310-f6099d32-9632-444c-bb03-52ad832d050a.png">

ì œê³± Jacobian normì„ ìµœì†Œí™”í•¨ìœ¼ë¡œì¨ inputì˜ ëª¨ë“  latent representationì€ ì„œë¡œ ë” ìœ ì‚¬í•œ ê²½í–¥ì´ ìˆìœ¼ë©°, representation ê°„ì˜ ì°¨ì´ê°€ ë” ì‘ê¸° ë•Œë¬¸ì— reconstructionì„ ë” ì–´ë µê²Œ ë§Œë“¬

reconstructionì— ì¤‘ìš”í•˜ì§€ ì•Šì€ latent representationì˜ variationì€ regularization ìš”ì¸ì— ì˜í•´ ê°ì†Œí•˜ëŠ” ë°˜ë©´, ì¤‘ìš”í•œ variationì€ reconstruction errorì— ëŒ€í•œ ì˜í–¥ ë•Œë¬¸ì— ë‚¨ì•„ ìˆì„ ê²ƒ



## 3 Variational Autoencoders

autoencoderì˜ í‘œí˜„ ëŠ¥ë ¥ì´ í¬ê²Œ í–¥ìƒëœ ê²ƒì€ Variational Autoencoders(VAE) ëª¨ë¸ ë•Œë¬¸-> Variational Bayes (VB) Inferenceì— ë”°ë¥´ë©´, VAEëŠ” í™•ë¥ ì  ë¶„í¬ë¥¼ í†µí•œ ë°ì´í„° ìƒì„±ì„ ì„¤ëª…í•˜ëŠ” generative model (í™•ë¥ ë¡ ì  decoderì™€ ë™ì¼)

<img width="301" alt="image" src="https://user-images.githubusercontent.com/60170358/172160854-ce1151e2-760c-4089-8a43-b16a71cece2d.png"> 

ê° data <img width="20" alt="image" src="https://user-images.githubusercontent.com/60170358/173175917-75d844ea-c600-422e-b4f2-d9b0f9f57364.png">ëŠ” ê´€ì°°ë˜ì§€ ì•Šì€ random latent variables <img width="19" alt="image" src="https://user-images.githubusercontent.com/60170358/173175931-545346f2-af45-440a-8bc8-0e63fe0c679c.png">ë¡œ ì¡°ê±´í™”ë˜ì–´ìˆë‹¤ê³  ê°€ì •í•˜ê³   ğœƒëŠ” generative ë¶„í¬ë¥¼ ì§€ë°°í•˜ëŠ” parameter <-> ëŒ€ì¹­ì ìœ¼ë¡œ, ê´€ì°°ëœ <img width="20" alt="image" src="https://user-images.githubusercontent.com/60170358/173175917-75d844ea-c600-422e-b4f2-d9b0f9f57364.png">ê°€ ì£¼ì–´ì§„ latent variable <img width="19" alt="image" src="https://user-images.githubusercontent.com/60170358/173175931-545346f2-af45-440a-8bc8-0e63fe0c679c.png">ì— ëŒ€í•œ ëŒ€ëµì ì¸ ì‚¬í›„ë¶„í¬ë¥¼ ê°€ì •í•˜ê³   ğœ™ëŠ” í™•ë¥ ë¡ ì  encoderì„ ì§€ë°°í•˜ëŠ” parameter

ë§ˆì§€ë§‰ìœ¼ë¡œ, <img width="42" alt="image" src="https://user-images.githubusercontent.com/60170358/172166110-a42fe869-bff0-4fee-923d-452c8f735480.png">ë¡œ í‘œì‹œëœ latent variables <img width="19" alt="image" src="https://user-images.githubusercontent.com/60170358/173175931-545346f2-af45-440a-8bc8-0e63fe0c679c.png">ëŒ€í•œ ì‚¬ì „ ë¶„í¬ë¥¼ ê°€ì •

ë§¤ê°œë³€ìˆ˜ ğœƒì™€ ğœ™ëŠ” ì•Œ ìˆ˜ ì—†ìœ¼ë©° ë°ì´í„°ì—ì„œ í•™ìŠµí•´ì•¼ í•¨



log-likelihood ëŠ” ê°ê° data pointsì˜ í•© <img width="209" alt="image" src="https://user-images.githubusercontent.com/60170358/173175709-42a98872-7a11-4103-93a8-fe20d7fec942.png"> ìœ¼ë¡œ í‘œí˜„ë˜ê³  ê°ê° data pointsë“¤ì€ ì‹ (7)ê³¼ ê°™ì´ ë‚˜íƒ€ëƒ„

<img width="367" alt="image" src="https://user-images.githubusercontent.com/60170358/173175534-2ed98baa-afa0-4dfe-82fb-c1210a0d9231.png">

ì²«ë²ˆì§¸ í•­ì€ KL divergenceì´ê³ , ë‘ë²ˆì§¸ í•­ì€ variational lower bound on the marginal likelihood(<img width="314" alt="image" src="https://user-images.githubusercontent.com/60170358/173177316-74b8a0b4-4031-4050-b8b2-ff1e078cb700.png">) 

-> KL divergenceëŠ” ìŒìˆ˜ê°€ ì•„ë‹ˆê¸° ë•Œë¬¸ì— lower boundë¥¼ ìµœëŒ€í™” í•˜ëŠ” ìª½ìœ¼ë¡œ í•´ì•¼ ì‚¬í›„ë¶„í¬ì˜ approximationì´ í–¥ìƒ(ë§¤ê°œë³€ìˆ˜ ğœƒì™€ ğœ™ë“¤ì„ ì¡°ì •í•˜ë©´ì„œ)



### 3.1 The Reparameterization Trick

 reparameterization trickì€ <img width="65" alt="image" src="https://user-images.githubusercontent.com/60170358/173177827-0f63fee0-7c08-4c87-acc3-4cce4cdfa377.png">ì„ ì¶”ì •í•˜ê¸° ìœ„í•œ ê°„ë‹¨í•œ approch : encoderì—ì„œ ë‚˜ì˜¨ í™•ë¥ ë¶„í¬ë¡œ ë°”ë¡œ samplingë¡œ í•˜ë©´ back propagationì´ ë¶ˆê°€ëŠ¥ (back propagationì€ í¸ë¯¸ë¶„ì„ êµ¬í•¨ìœ¼ë¡œ gradientë¥¼ êµ¬í•˜ëŠ” ê²ƒì¸ë°, zë¥¼ í™•ë¥ ë¶„í¬ì—ì„œ ê·¸ëƒ¥ sampling í•œë‹¤ë©´ chain ruleì´ ì¤‘ê°„ì— ëŠê¸°ê²Œ ë¨, ê·¸ë˜ì„œ reparameterization trickì‚¬ìš©) 

<img width="374" alt="image" src="https://user-images.githubusercontent.com/60170358/173178083-e3be983f-4275-4ae8-8897-844b9d37a932.png">

<img width="337" alt="image" src="https://user-images.githubusercontent.com/60170358/173178720-a19d5d6f-d8c8-48c3-b051-058be1e516c2.png">

variational loss functionì„ ìœ„ì˜ ì‹ (13)ê³¼ ê°™ì´ ë°”ê¿”ì¤Œ



### 3.2 Example: The Case of Normal Distribution

<img width="48" alt="image" src="https://user-images.githubusercontent.com/60170358/173178931-29865ed8-70ac-4d9b-a01a-fd951cb77c7b.png">ë¥¼ gaussian ë¶„í¬ë¡œ ëª¨ì‚¬í•˜ëŠ”ê²Œ ë³´í†µì ì´ì§€ë§Œ, 

<img width="282" alt="image" src="https://user-images.githubusercontent.com/60170358/173179005-3a4486e8-37ad-4dcc-8653-477f0e80506d.png">ë¥¼ í†µí•´  reparametrisation trick ë¥¼<img width="63" alt="image" src="https://user-images.githubusercontent.com/60170358/173179016-b61d39cd-ce60-4de4-8c09-77bfdbb5bd6b.png">ê°€ëŠ¥í•˜ê²Œ í•´ì•¼í•¨

ì´ë•Œ <img width="63" alt="image" src="https://user-images.githubusercontent.com/60170358/173179016-b61d39cd-ce60-4de4-8c09-77bfdbb5bd6b.png">ì€ normal distributionì„

![image-20220611170221631](C:\Users\chaeh\AppData\Roaming\Typora\typora-user-images\image-20220611170221631.png)

ì „ì²´ networkì˜ loss functionì€ ì‹ (15)ì™€ ê°™ê²Œ ë¨



### 3.3 Disentangled Autoencoders

<img width="373" alt="image" src="https://user-images.githubusercontent.com/60170358/173179638-f47478e3-7e71-4dfa-9441-c0c848e453c4.png">

ì‹ (9)ì—ì„œ ì˜¤ë¥¸ìª½ í•­ì€ sampleì˜ reconstruction ëŠ¥ë ¥ì„ ë‚˜íƒ€ë‚´ê³ , ì™¼ìª½ í•­ì€ regularizationìœ¼ë¡œ í–‰ë™í•¨	

disentangled autoencoderì€ varational autoencoderì„ í¬í•¨í•¨->  ğ›½ëŠ” KL divergenceì˜ ê³±ì…ˆ ì¸ìë¡œ ì¶”ê°€ë¨

![image-20220611172110746](C:\Users\chaeh\AppData\Roaming\Typora\typora-user-images\image-20220611172110746.png)

ìµœëŒ€í™” ê³„ìˆ˜ëŠ” ì‹ (16)ê³¼ ê°™ìŒ

ì‹¤ì œë¡œ, ì‚¬ì „ <img width="35" alt="image" src="https://user-images.githubusercontent.com/60170358/173179905-6f1406bc-060c-40ed-bbf4-77de72b71e1e.png">ì€ ì¼ë°˜ì ìœ¼ë¡œ standard multivariate normal distributionìœ¼ë¡œ ì„¤ì •ë¨ 

-> ëª¨ë“  featureì€ ìƒê´€ ê´€ê³„ê°€ ì—†ìœ¼ë©°, KL divergenceëŠ” latent feature distributionì„ ìƒê´€ê´€ê³„ê°€ ë‚®ì€ ê²ƒìœ¼ë¡œ ê·œì œí•¨



## 4 Applications of autoencoders

autoencoderì„ í†µí•´ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ê²ƒì€ ë‹¤ì–‘í•œ ì‘ìš© í”„ë¡œê·¸ë¨ì— ì‚¬ìš©ë  ìˆ˜ ìˆìŒ



### 4.1 Autoencoders as a generative model

variational autoencoderëŠ” í™•ë¥ ì  ë¶„í¬ë¥¼ í†µí•œ ë°ì´í„° ìƒì„±ì„ ì„¤ëª…í•˜ëŠ” generative model

<img width="379" alt="image" src="https://user-images.githubusercontent.com/60170358/173185555-1e5e0b6b-fa63-4da1-9302-c73d01a3f2fd.png">

variational autoencoderì— ì˜í•´ ìƒì„±ëœ ì´ë¯¸ì§€ì´ê³ , MNIST ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ í•™ìŠµ



### 4.2 Use of autoencoders for classification

autoencoderëŠ” unsupervised ë°©ì‹ìœ¼ë¡œ ìœ¼ë¡œ í›ˆë ¨ë˜ê³  ìˆì§€ë§Œ, ë¶„ë¥˜ ê²°ê³¼ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ semi-supervisedì—ì„œë„ ì‚¬ìš©ë¨

1) autoencoderëŠ” ì´ì „ ì„¹ì…˜ì—ì„œ ì„¤ëª…í•œ ê²ƒì²˜ëŸ¼ ê°ë…ë˜ì§€ ì•Šì€ ë°©ì‹ìœ¼ë¡œ í›ˆë ¨->  decoderì„ í•œìª½ì— ë‘ê³  encoderì„ ë¶„ë¥˜ ëª¨ë¸ì˜ ì²« ë²ˆì§¸ ë¶€ë¶„ìœ¼ë¡œ ì‚¬ìš©

   support vector machine ì¸ì½”ë”ì˜ ì¶œë ¥ì„ í›ˆë ¨

   domainì´ high dimensional ì´ê³  layer-by-layer trainingì´ ë¶ˆê°€ëŠ¥í•˜ë©´, non linearity ë¥¼ ì¶”ê°€í•˜ê¸° ì „ì— ê° ê³„ì¸µì„ linear layerë¡œ í›ˆë ¨

2) classification newtorkì— ëŒ€í•œ regularization ê¸°ìˆ ë¡œ autoencoderì„ ì‚¬ìš©

   classification newtork(ë¼ë²¨ì´ ì§€ì •ëœ ë°ì´í„°ë¡œ í›ˆë ¨ë¨)ì™€ decoder network(ë¼ë²¨ì´ ì§€ì •ëœ ë°ì´í„° ë˜ëŠ” ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ ë°ì´í„°ë¥¼ ì¬êµ¬ì„±í•˜ë„ë¡ í›ˆë ¨ë¨)

   <img width="363" alt="image" src="https://user-images.githubusercontent.com/60170358/173186206-eeca6e15-6a9d-47db-900b-ab9555f85ab5.png">

   autoencoderì„ regularizationìœ¼ë¡œ ì‚¬ìš©



### 4.3 Use of autoencoders for clustering

cluasteringì€ ê° ê·¸ë£¹ì˜ ìƒ˜í”Œì´ ì„œë¡œ ìœ ì‚¬í•˜ê³  ë‹¤ë¥¸ ê·¸ë£¹ì˜ ìƒ˜í”Œê³¼ ë‹¤ë¥´ë„ë¡ ë°ì´í„°ë¥¼ ê·¸ë£¹ìœ¼ë¡œ ë¶„í• í•˜ëŠ” ê²ƒì´ ëª©í‘œì¸ unsupervised task-> ëŒ€ë¶€ë¶„ì˜ clustering algorithmì€ ë°ì´í„°ì˜ ì°¨ì›ì— ë¯¼ê°í•˜ê³  ì°¨ì›ì˜ ì €ì£¼ ë¬¸ì œ ì¡´ì¬

ë°ì´í„°ì— low-dimensional latent representationì´ ìˆë‹¤ê³  ê°€ì •í•˜ë©´, ë” ì ì€ featureë¡œ êµ¬ì„±ëœ ë°ì´í„°ì— ëŒ€í•œ í‘œí˜„ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ autoencoderì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ

autoencoderëŠ” ë¶„ë¥˜ì—ì„œì˜ ì‚¬ìš©ê³¼ ìœ ì‚¬í•˜ê²Œ decoderëŠ” í•œìª½ì— ë†“ì´ê³ , ê° ë°ì´í„° í¬ì¸íŠ¸ì˜ latent representation(encoder output)ì´ ìœ ì§€ë˜ê³  ì£¼ì–´ì§„ clustering(K-means)ì— ëŒ€í•œ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë¨



### 4.4 Use of autoencoders for anomaly detection

anomaly detectionì€ unsupervised task-> ì •ìƒ ë°ì´í„° ì˜ˆì œë§Œ ì£¼ì–´ì§„ ì •ìƒ profileì„ í•™ìŠµí•œ ë‹¤ìŒ ì •ìƒ profileì— ë§ì§€ ì•ŠëŠ” sampleì„ ì´ìƒ ì§•í›„ë¡œ ì‹ë³„

ë¶€ì • í–‰ìœ„ íƒì§€, ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ë“±ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì‘ìš© í”„ë¡œê·¸ë¨ì— ì ìš©ë  ìˆ˜ ìˆìŒ



### 4.5 Use of autoencoders for recommendation systems

ì¶”ì²œ ì‹œìŠ¤í…œ(Recommendor System)ì€ ì‚¬ìš©ì ì„ í˜¸ë„ ë˜ëŠ” ì„ í˜¸ë„ë¥¼ ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ëª¨ë¸ ë˜ëŠ” ì‹œìŠ¤í…œ

Recommendor Systemì€ ì „ì ìƒê±°ë˜ ì›¹ ì‚¬ì´íŠ¸, ì• í”Œë¦¬ì¼€ì´ì…˜ ìŠ¤í† ì–´ë“±ì— ì‚¬ìš©-> ê¶Œì¥ ì‹œìŠ¤í…œ ëª¨ë¸ì˜ ê³ ì „ì ì¸ ì ‘ê·¼ ë°©ì‹ì€ Collaborative Filtering(CF) : CFì—ì„œ ì‚¬ìš©ì ê¸°ë³¸ ì„¤ì •ì€ ë‹¤ë¥¸ ì‚¬ìš©ì ê¸°ë³¸ ì„¤ì •ì˜ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡ (ê³¼ê±°ì— ìœ ì‚¬í•œ ì„ í˜¸ë„ë¥¼ ë³´ì¸ ì‚¬ëŒë“¤ì€ ë¯¸ë˜ì— ë¹„ìŠ·í•œ ì„ í˜¸ë„ë¥¼ ë³´ì¼ ê²ƒ)

ì¶”ì²œ ì‹œìŠ¤í…œìš© autoencoderì€ AutoRec model : user-based AutoRec/item-based AutoRec ë‘ê°€ì§€ê°€ ì¡´ì¬: U-AutoRecì€ íŠ¹ì • ì‚¬ìš©ìì— ëŒ€í•œ lower dimensional representationì„ í•™ìŠµí•˜ëŠ” ë°˜ë©´, I-AutoRecì—ì„œëŠ” autoencoder íŠ¹ì • í•­ëª©ì— ëŒ€í•œ lower dimensional representationì„ í•™ìŠµ



### 4.6 Use of autoencoders for dimensionality reduction

í…ìŠ¤íŠ¸ë‚˜ ì´ë¯¸ì§€ì™€ ê°™ì€ real dataëŠ” sparse high-dimensional representationì„ ì‚¬ìš©-> ì°¨ì›ì„±ì˜ ì €ì£¼ë¡œ ì´ì–´ì§

ì°¨ì› ì¶•ì†Œì˜ ëª©í‘œëŠ” lower dimensional manifold("intrinsic dimensionalisty") ê³µê°„ì„ ë°°ìš°ëŠ”ê²ƒ

Principal Component Analysis(PCA): data pointë¥¼ ì €ì°¨ì› ê³µê°„ì— ì„ í˜• íˆ¬ì˜í•˜ì—¬ sqaure reconstruct lossë¥¼ ìµœì†Œí™”

ì„œë¡œ ë‹¤ë¥¸ objectivesë¥¼ ì‚¬ìš©: Linear Discriminant Analysis(LDA)ì€ ì„œë¡œ ë‹¤ë¥¸ í´ë˜ìŠ¤ì˜ ë°ì´í„°ë¥¼ êµ¬ë³„í•˜ëŠ” ë° ê°€ì¥ ì í•©í•œ ì„ í˜• ë¶€ë¶„ ê³µê°„ì„ ì°¾ëŠ” supervised method, ISOMAPì€ ì›ë˜ ê³µê°„ì—ì„œ ìŒë³„ ë°ì´í„° ì‚¬ì´ì˜ geodesic distanceë¥¼ ìœ ì§€í•˜ì—¬ ì €ì°¨ì› ë§¤ë‹ˆí´ë“œë¥¼ í•™ìŠµ



## 5  Advanced autoencoder techniques

autoencoderì€ ì¼ë°˜ì ìœ¼ë¡œ ì…ë ¥ê³¼ ì¶œë ¥ ì‚¬ì´ì˜ ì°¨ì´ì— í•´ë‹¹í•˜ëŠ” loss functionì— ì˜í•´ í›ˆë ¨-> ìœ„ì™€ ê°™ì´ autoencoderì˜ ì¥ì  ì¤‘ í•˜ë‚˜ëŠ” ë‹¤ì–‘í•œ ìš©ë„ì— latent representationì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ



### 5.1 Autoencoders and generative adversarial networks

Variational auto encoderëŠ” ì•½ê°„ íë¦¿í•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” MSEì—ì„œ í›ˆë ¨ë˜ì§€ë§Œ ì¶œë ¥ì„ ì œì–´í•˜ê¸° ìœ„í•´ latent variable ì¶”ë¡ ì„ í—ˆìš©

ë°ì´í„°ë¥¼ í•©ì„±í•˜ëŠ” ìë™ ì¸ì½”ë”ì˜ ëŒ€ì²´ ìƒì„± ëª¨ë¸ì€ GAN(Generative Adversarial Networks)-> generatorê³¼ discriminatorëŠ” ì„œë¡œ ê²½ìŸí•˜ë„ë¡ ê°•ì œí•˜ëŠ” loss functionì„ ì‚¬ìš©í•´ í•¨ê»˜ í›ˆë ¨ë˜ì–´ ìƒì„± ë°ì´í„°ì˜ í’ˆì§ˆì„ í–¥ìƒì‹œí‚´

adversarial autoencoderì—ì„œ, VAE ì†ì‹¤ í•¨ìˆ˜ì˜ KL divergenceëŠ” ì‚¬ì „ê³¼ ì¶”ì •ëœ ì‚¬í›„ë¥¼ êµ¬ë³„í•˜ëŠ” discriminatorë¡œ ëŒ€ì²´ë¨

VAE lossì˜ reconstruction lossëŠ” discriminatorë¡œ ëŒ€ì²´ë˜ë©°, ì´ëŠ” decoderì„ generatorì™€ ë³‘í•©í•˜ë„ë¡ í•¨

GANì˜ discriminatorì€ shared weightsë¥¼ í†µí•´ encoderì™€ ê²°í•©ë˜ì–´ GMMì´ ì¶”ë¡ ì„ ìœ„í•´ latent spaceì„ í¸ë¦¬í•˜ê²Œ ëª¨ë¸ë§í•¨



### 5.2 Adversarially learned inference

GANì˜ ë‹¨ì  ì¤‘ í•˜ë‚˜ëŠ” mode collapseì¸ë°, autoencoderì™€ ë‹¬ë¦¬ ëª¨ë“  ë°ì´í„°ê°€ ì•„ë‹Œ ë°ì´í„°ì˜ ì¼ë¶€ë§Œ latent spaceë¥¼ í†µí•´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŒ

Adversarially Learned Inference(ALI)ì—ëŠ” VAEì™€ GANSì˜ ì•„ì´ë””ì–´ë¥¼ ë³‘í•©í•˜ì—¬ ì¥ë‹¨ì ì„ ì ˆì¶©í•˜ë ¤ëŠ” ì‹œë„-> decoderë¥¼ "ì†ì´ê¸°" ìœ„í•´ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ë„ë¡ ê°•ì œë¨<img width="439" alt="image" src="https://user-images.githubusercontent.com/60170358/173190907-349ef34a-db4b-4af0-8ae3-b30243089934.png">

ALIê°€ ì´ë¯¸ì§€ì—ì„œ ì˜ë¯¸ ìˆëŠ” ë³€ê²½ì„ ì–»ê¸° ìœ„í•´ íŠ¹ì • featureì„ ë³€ê²½í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ì˜ˆ(ëª¨ë¸ì€ CelebA ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ í›ˆë ¨)

-> ê° ì´ë¯¸ì§€ì— 40ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ì†ì„±ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©°, ALIì—ëŠ” encoder, decoder, discriminatorì— ì„ í˜•ì ìœ¼ë¡œ í¬í•¨

-> ê° í–‰ì—ëŠ” ì—¬ëŸ¬ ì—´ì— ê±¸ì³ ì¼ì •í•˜ê²Œ ìœ ì§€ë˜ëŠ” ì†ì„±ì˜ ë¶€ë¶„ ì§‘í•©ì´ ì¡´ì¬



### 5.3 Wasserstein autoencoders

GANì€ ì´ë¯¸ì§€ë¥¼ generateí•˜ì§€ë§Œ inferenceë¥¼ ì œê³µí•˜ì§€ ì•Šìœ¼ë©° í•™ìŠµ ì•ˆì •ì„±ê³¼ ê´€ë ¨í•˜ì—¬ ë§ì€ ê³ ìœ í•œ ë¬¸ì œë¥¼ ê°–ìŒ

Wasserstein-GAN(WGAN)ì€ Wasserstein distanceë¥¼ ì‚¬ìš©í•˜ì—¬ ë§ì€ ë¬¸ì œë¥¼ í’€ ìˆ˜ ìˆìŒ

Wasserstein distanceëŠ” Optimal Transport distanceì˜ íŠ¹ë³„í•œ ê²½ìš°ì´ê³  <img width="29" alt="image" src="https://user-images.githubusercontent.com/60170358/173232159-688e3c22-4e0e-4a39-a9a2-9296cd5cb173.png">ì™€ <img width="23" alt="image" src="https://user-images.githubusercontent.com/60170358/173232168-1bf11efe-c687-4bab-8b10-a03ecd2260d5.png">ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ì¬ëŠ” ê²ƒ

<img width="330" alt="image" src="https://user-images.githubusercontent.com/60170358/173232189-5f6bfcfe-5fa3-4783-85bf-438e4e212e2c.png">

c(X,Y)ëŠ” cost function, p-th root of WcëŠ” p-Wassertein distanceë¼ê³  ë¶€ë¦„

WAE(Wasserstein autoencoders)ì—ì„œ autoencoderì˜ loss functionì„ ìˆ˜ì •í•˜ì—¬ ì‹(21) ê³¼ ê°™ì€ objective functionì„ ë§Œë“¬

<img width="400" alt="image" src="https://user-images.githubusercontent.com/60170358/173233330-1784012b-3738-4b39-bf79-04d0af8521f7.png">

-> QëŠ” encoderì´ê³  GëŠ” decoder

-> ì™¼ìª½ ë¶€ë¶„ì€ reconstruction lossë¡œ output ë¶„í¬ì™€ sample ë¶„í¬ ì‚¬ì´ë¥¼ penalize / ì˜¤ë¥¸ìª½ ë¶€ë¶„ì€ latent space ë¶„í¬ì—ì„œ ì‚¬ì „ ë¶„í¬ ì‚¬ì´ë¥¼ penalize

<img width="455" alt="image" src="https://user-images.githubusercontent.com/60170358/173233573-07c3ecd1-92f3-4b65-8ecd-6d48580cc945.png">

-> VAEì™€ WAE ì‚¬ì´ì˜ ì •ê·œí™” ì°¨ì´

-> VAEì™€ WAEëŠ” 2ê°€ì§€ termì„ ìµœì†Œí™”: reconstruction costì™€ regularizer penalizing  discrepancy(encoder Që¡œ ìœ ë„ëœ ë¶„í¬ì™€ Pzì‚¬ì´)

-> ê·¸ë¦¼ (a) : VAEëŠ” ğ‘„(ğ‘|ğ‘‹ = ğ‘¥) ë¥¼ ê°•ì œë¡œ Pxì—ì„œ ë„ì¶œëœ ëª¨ë“  ì…ë ¥ ì˜ˆì— ëŒ€í•´ Pzì™€ ì¼ì¹˜ì‹œí‚´  -> ëª¨ë“  ë¹¨ê°„ìƒ‰ ê³µì€ í°ìƒ‰ ëª¨ì–‘ìœ¼ë¡œ í‘œì‹œëœ Pzì™€ ì¼ì¹˜í•´ì•¼í•¨ ë¹¨ê°„ ê³µë“¤ì˜ intersectingìœ¼ë¡œ ì¸í•´ reconstruction ë¬¸ì œê°€ ìƒê¹€

-> ê·¸ë¦¼ (b) : WAEëŠ” Pzì™€ ì¼ì¹˜ì‹œí‚¤ê¸° ìœ„í•´ ì—°ì†ì ì¸ mixture ğ‘„z:= âˆ« ğ‘„(ğ‘|ğ‘‹)ğ‘‘ğ‘ƒx ë¥¼ ì‚¬ìš©-> ì´ˆë¡ìƒ‰ ê³µìœ¼ë¡œ í‘œì‹œë¨, ì„œë¡œ ë‹¤ë¥¸ exmapleì˜ latent codeëŠ”  ì„œë¡œ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆê²Œ ë˜ì–´ ë” ë‚˜ì€ reconstructionì„ ì´‰ì§„



### 5.4 Deep feature consistent variational autoencoder

autoencoderì„ ìµœì í™”í•˜ê¸° ìœ„í•´ ë‹¤ë¥¸ loss function ì œì‹œ: ì›ë³¸ ì˜ìƒê³¼ reconstruction ì˜ìƒì´ ì£¼ì–´ì§€ë©´ í”½ì…€ ì°¨ì´ì—ì„œ normì„ ì¸¡ì •í•˜ëŠ” ëŒ€ì‹  í”½ì…€ ê°„ì˜ ìƒê´€ ê´€ê³„ë¥¼ ê³ ë ¤í•˜ëŠ” ë‹¤ë¥¸ ì¸¡ì •ì´ ì‚¬ìš©ë¨

Pretrained classification networkëŠ” autoencoderì— ëŒ€í•œ loss functionì„ ìƒì„±í•˜ëŠ”ë° ì‚¬ìš©í•¨-> ì˜ìƒì„ encoding ë° decoding í•œ í›„ originalê³¼ reconstructed ì˜ìƒì´ ëª¨ë‘ pretrained networkì˜ inputìœ¼ë¡œ ì‚½ì…ë¨

pretrained network ê²°ê³¼ê°€ ë†’ì€ ì •í™•ë„ë¡œ ë‚˜íƒ€ë‚˜ê³  í•™ìŠµëœ ë„ë©”ì¸ì´ autoencoderì˜ ë„ë©”ì¸ê³¼ í¬ê²Œ ë‹¤ë¥´ì§€ ì•Šë‹¤ê³  ê°€ì •í•˜ë©´, ê° layerëŠ” input imageì˜ ì„±ê³µì ì¸ feature extractorë¡œ ë³¼ ìˆ˜ ìˆê³ ,  ë‘ ì´ë¯¸ì§€ ê°„ì˜ ì°¨ì´ë¥¼ ì§ì ‘ ì¸¡ì •í•˜ëŠ” ëŒ€ì‹  network layerì—ì„œ ë‘ ì´ë¯¸ì§€ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆìŒ



### 5.5 Conditional image generation with PixelCNN decoders

autoencoderì™€ PixelCNNì‚¬ì´ì˜ êµ¬ì„±ì„ ì œì•ˆ-> PixelCNNì—ì„œ ì´ë¯¸ì§€ì˜ í”½ì…€ì€ ì„ì˜ì˜ ìˆœì„œì— ë”°ë¼ ì •ë ¬ë˜ê³ ,ê° í”½ì…€ì´ ì´ì „ í”½ì…€ì˜ ì¶œë ¥ê³¼ ì…ë ¥ì˜ ê²°ê³¼ì¸ ì¶œë ¥ì´ ìˆœì°¨ì ìœ¼ë¡œ í˜•ì„±ë¨

imageì˜ local sptial statisticsë¥¼ ê³ ë ¤í•¨(ì˜ˆë¥¼ ë“¤ì–´, ë°°ê²½ í”½ì…€ ì•„ë˜ì— ë‹¤ë¥¸ ë°°ê²½ í”½ì…€ì„ ê°€ì§ˆ í™•ë¥ ì´ ì „ê²½ í”½ì…€ì„ ê°€ì§ˆ í™•ë¥ ë³´ë‹¤ ë†’ìŒ): ì…ë ¥ í”½ì…€ ì •ë³´ ì™¸ì— spatial orderingì„ ì‚¬ìš©í•˜ë©´ íë¦¿í•œ í”½ì…€ì„ ì–»ì„ ê°€ëŠ¥ì„±ì´ ì¤„ì–´ë“¬

ì´í›„ì—, local statisticsëŠ” RNNì˜ ì‚¬ìš©ìœ¼ë¡œ ëŒ€ì²´ë˜ì—ˆì§€ë§Œ, í”½ì…€ ìƒì„±ì— ëŒ€í•œ ë™ì¼í•œ ê°œë…ì€ ë‚¨ìŒ(decoderì„ ìˆœì°¨ì ìœ¼ë¡œ ì¶œë ¥ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” í”½ì…€ CNN networkë¡œ êµ¬ì„±í•˜ë„ë¡ ì„¤ì •í•˜ì—¬ autoencoderì™€ ê²°í•©)

<img width="318" alt="image" src="https://user-images.githubusercontent.com/60170358/173235224-a9daeb19-e911-4665-bd97-95c424223769.png">

-> ìœ„-> ì•„ë˜, ì˜¤ë¥¸ìª½-> ì™¼ìª½ìœ¼ë¡œ ìƒì„±ë˜ëŠ” pixelCNN generation network (ë‹¤ìŒì— ìƒì„±ë˜ëŠ” í”½ì…€ì€ ë…¸ë€ìƒ‰ í”½ì…€ì´ê³ , RNNì€ ìˆ¨ê²¨ì§„ ìƒíƒœì™€ ë¹¨ê°„ìƒ‰ ì‚¬ê°í˜•ì˜ ë…¹ìƒ‰ í”½ì…€ ì •ë³´ë¥¼ ê³ ë ¤)



## 6 Conclusion

ì²˜ìŒì— ì •ì˜ëœ architectureê°€ ì…ë ¥ì˜ meaningful representationì„ í•™ìŠµí•˜ê³  generative processë¥¼ ëª¨ë¸ë§í•˜ëŠ” í•µì‹¬ ëŠ¥ë ¥ì„ ê°€ì§„ ê°•ë ¥í•œ ëª¨ë¸ë¡œ ì–´ë–»ê²Œ ì§„í™”í–ˆëŠ”ì§€ë¥¼ ë³´ì—¬ì£¼ëŠ” autoencoderì„ ì œì‹œ

autoencoder fall backsì¤‘ í•˜ë‚˜ëŠ”, reconstruction errorì— ì¶œë ¥ì´ ì–¼ë§ˆë‚˜ í˜„ì‹¤ì ì¸ì§€ í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒ

generative prossesë¥¼ ëª¨ë¸ë§í•˜ëŠ” ê²½ìš°, variational ê³¼ disentangled autoencoderì˜ ì„±ê³µì—ë„ ë¶ˆêµ¬í•˜ê³  hidden stateì˜ sizeì™€ distributionì„ ì„ íƒí•˜ëŠ” ë°©ë²•ì€ ì‹¤í—˜ê¸°ë°˜ì´ë©°, reconstruction errorë¥¼ ê³ ë ¤í•˜ê³ , ì‚¬í›„ í›ˆë ¨ì—ì„œ hidden stateë¥¼ ë³€í™”ì‹œí‚´ -> ì´ëŸ¬í•œ parameterì„ ë” ì˜ ì„¤ì •í•˜ëŠ” í–¥í›„ ì—°êµ¬ê°€ í•„ìš”

**autoencoderì˜ ëª©í‘œëŠ” compressed and meaningful í•œ representationì„ ì–»ëŠ” ê²ƒ**
